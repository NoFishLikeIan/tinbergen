% Encoding: UTF-8

@Article{Bousquet2020,
  author      = {Olivier Bousquet and Steve Hanneke and Shay Moran and Ramon van Handel and Amir Yehudayoff},
  title       = {A Theory of Universal Learning},
  abstract    = {How quickly can a given class of concepts be learned from examples? It is common to measure the performance of a supervised machine learning algorithm by plotting its "learning curve", that is, the decay of the error rate as a function of the number of training examples. However, the classical theoretical framework for understanding learnability, the PAC model of Vapnik-Chervonenkis and Valiant, does not explain the behavior of learning curves: the distribution-free PAC model of learning can only bound the upper envelope of the learning curves over all possible data distributions. This does not match the practice of machine learning, where the data source is typically fixed in any given scenario, while the learner may choose the number of training examples on the basis of factors such as computational resources and desired accuracy. In this paper, we study an alternative learning model that better captures such practical aspects of machine learning, but still gives rise to a complete theory of the learnable in the spirit of the PAC model. More precisely, we consider the problem of universal learning, which aims to understand the performance of learning algorithms on every data distribution, but without requiring uniformity over the distribution. The main result of this paper is a remarkable trichotomy: there are only three possible rates of universal learning. More precisely, we show that the learning curves of any given concept class decay either at an exponential, linear, or arbitrarily slow rates. Moreover, each of these cases is completely characterized by appropriate combinatorial parameters, and we exhibit optimal learning algorithms that achieve the best possible rate in each case. For concreteness, we consider in this paper only the realizable case, though analogous results are expected to extend to more general learning scenarios.},
  date        = {2020-11-09},
  eprint      = {2011.04483},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/2011.04483v1:PDF},
  keywords    = {cs.LG, cs.DS, math.ST, stat.ML, stat.TH, prio3},
  priority    = {prio3},
}

@Article{Friston2010,
  author    = {Karl Friston},
  journal   = {Nature Reviews Neuroscience},
  title     = {The free-energy principle: a unified brain theory?},
  year      = {2010},
  month     = {jan},
  number    = {2},
  pages     = {127--138},
  volume    = {11},
  doi       = {10.1038/nrn2787},
  keywords  = {prio3},
  priority  = {prio3},
  publisher = {Springer Science and Business Media {LLC}},
}

@InProceedings{Schwarz1992,
  author    = {Reinhard Schwarz},
  booktitle = {Proceedings of the 5th workshop on {ACM} {SIGOPS} European workshop Models and paradigms for distributed systems structuring - {EW} 5},
  title     = {Causality in distributed systems},
  year      = {1992},
  publisher = {{ACM} Press},
  doi       = {10.1145/506378.506423},
  file      = {:C\:/Users/andre/Dropbox/random_papers/casuality in distributed sys.pdf:PDF},
  keywords  = {prio3},
  priority  = {prio3},
}

@Article{Lanctot2017,
  author      = {Marc Lanctot and Vinicius Zambaldi and Audrunas Gruslys and Angeliki Lazaridou and Karl Tuyls and Julien Perolat and David Silver and Thore Graepel},
  title       = {A Unified Game-Theoretic Approach to Multiagent Reinforcement Learning},
  abstract    = {To achieve general intelligence, agents must learn how to interact with others in a shared environment: this is the challenge of multiagent reinforcement learning (MARL). The simplest form is independent reinforcement learning (InRL), where each agent treats its experience as part of its (non-stationary) environment. In this paper, we first observe that policies learned using InRL can overfit to the other agents' policies during training, failing to sufficiently generalize during execution. We introduce a new metric, joint-policy correlation, to quantify this effect. We describe an algorithm for general MARL, based on approximate best responses to mixtures of policies generated using deep reinforcement learning, and empirical game-theoretic analysis to compute meta-strategies for policy selection. The algorithm generalizes previous ones such as InRL, iterated best response, double oracle, and fictitious play. Then, we present a scalable implementation which reduces the memory requirement using decoupled meta-solvers. Finally, we demonstrate the generality of the resulting policies in two partially observable settings: gridworld coordination games and poker.},
  date        = {2017-11-02},
  eprint      = {1711.00832},
  eprintclass = {cs.AI},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1711.00832v2:PDF;:C\:/Users/andre/Dropbox/random_papers/unified-reinf-learning.pdf:PDF},
  keywords    = {cs.AI, cs.GT, cs.LG, cs.MA, prio3},
  priority    = {prio3},
}

@Article{Frankhauser1998,
  author    = {Pierre Frankhauser},
  journal   = {Discrete Dynamics in Nature and Society},
  title     = {Fractal geometry of urban patterns and their morphogenesis},
  year      = {1998},
  number    = {2},
  pages     = {127--145},
  volume    = {2},
  doi       = {10.1155/s1026022698000107},
  keywords  = {prio3},
  priority  = {prio3},
  publisher = {Hindawi Limited},
}

@Comment{jabref-meta: databaseType:bibtex;}
